import logging
import os
import shutil
import struct
import tempfile

import imageio.v3 as iio

from generated.formats.dds import DdsFile
from generated.formats.dds.enums.DxgiFormat import DxgiFormat
from generated.formats.ovl.versions import *
from generated.formats.tex.compounds.TexHeader import TexHeader
from modules.formats.BaseFormat import MemStructLoader, BaseFile
from modules.helpers import split_path

from ovl_util import texconv, imarray


def align_to(width, comp, alignment=64):
	"""Return input padded to the next closer multiple of alignment"""
	# get bpp from compression type
	if "BC1" in comp or "BC4" in comp:
		alignment *= 2
	# print("alignment",alignment)
	m = width % alignment
	if m:
		return width + alignment - m
	return width


class TexturestreamLoader(BaseFile):
	extension = ".texturestream"

	def create(self):
		self.create_root_entry()
		if is_jwe2(self.ovl):
			lod_index = int(self.file_entry.basename[-1])
			root_data = struct.pack("<QQ", 0, lod_index)
		else:
			# JWE1, PZ, PC
			root_data = struct.pack("<Q", 0)
		self.write_data_to_pool(self.root_entry.struct_ptr, 3, root_data)
		# data entry, assign buffer
		self.create_data_entry((b"", ))


class DdsLoader(MemStructLoader):
	target_class = TexHeader
	extension = ".tex"
	temp_extensions = (".dds", )

	def link_streams(self):
		"""Collect other loaders"""
		self._link_streams(f"{self.file_entry.basename}_lod{lod_i}.texturestream" for lod_i in range(3))

	def increment_buffers(self, loader, buffer_i):
		"""Linearly increments buffer indices for games that need it"""
		# create increasing buffer indices for PZ (still needed 22-05-10), JWE1
		if not is_jwe2(self.ovl):
			for buff in loader.data_entry.buffers:
				buff.index = buffer_i
				buffer_i += 1
		return buffer_i

	def create(self):
		name_ext, name, ext = split_path(self.file_entry.path)
		super().create()
		logging.debug(f"Creating image {name_ext}")
		# there's one empty buffer at the end!
		buffers = [b"" for _ in range(self.header.stream_count + 1)]
		# decide where to store the buffers
		static_lods = 2
		streamed_lods = len(buffers) - static_lods
		logging.info(f"buffers: {len(buffers)} streamed lods: {streamed_lods}")
		buffer_i = 0
		# generate ovs and lod names - highly idiosyncratic
		# checked for PZ, JWE2, PC
		if streamed_lods == 0:
			indices = ()
		elif streamed_lods == 1:
			# 1 lod: lod0 -> L1
			indices = ((0, 1),)
		elif streamed_lods == 2:
			# 2 lods: lod0 -> L0, lod1 -> L1
			indices = ((0, 0), (1, 1), )
		else:
			raise IndexError(f"Don't know how to handle more than 2 streams for {name_ext}")
		for lod_i, ovs_i in indices:
			ovs_name = f"Textures_L{ovs_i}"
			# create texturestream file - dummy_dir is ignored
			texstream_loader = self.ovl.create_file(f"dummy_dir/{name}_lod{lod_i}.texturestream", ovs_name=ovs_name)
			self.streams.append(texstream_loader)
			buffer_i = self.increment_buffers(texstream_loader, buffer_i)
		self.create_data_entry(buffers[streamed_lods:])
		self.increment_buffers(self, buffer_i)
		# ready, now inject
		self.load_image(self.file_entry.path)

	def load_image(self, file_path):
		logging.debug(f"Loading image {file_path}")
		tmp_dir = tempfile.mkdtemp("-cobra-tools")
		png_path = imarray.png_from_tex(file_path, tmp_dir)
		dds_path = f"{os.path.splitext(file_path)[0]}.dds"
		if png_path:
			self.load_png(png_path, tmp_dir)
		elif os.path.isfile(dds_path):
			self.load_dds(dds_path)
		else:
			raise FileNotFoundError(f"Found no associated image files for {file_path}")
		shutil.rmtree(tmp_dir)

	def load_png(self, file_path, tmp_dir):
		logging.info(f"Loading PNG {file_path}")
		# convert the png into a dds, then inject that
		size_info = self.get_tex_structs()
		compression = self.header.compression_type.name
		# need to check the png dimensions here because texconv writes weird things to the dds header
		self.ensure_size_match(file_path, size_info)
		# we need to use tricks to make texconv generate proper mip maps for array textures
		# save stacked png as uncompressed dds
		dds_file_path = texconv.png_to_uncompressed_dds(
			file_path, size_info.height * size_info.array_size, tmp_dir, codec=compression)
		# correct the dimensions
		dds_file = DdsFile()
		dds_file.load(dds_file_path)
		dds_file.height = size_info.height
		dds_file.dx_10.array_size = size_info.array_size
		dds_file.save(dds_file_path)
		# compress and generate mips if needed
		dds_file_path = texconv.add_mips_to_dds(dds_file_path, tmp_dir, codec=compression, mips=size_info.num_mips)

		# inject the dds generated by texconv
		self.load_dds(dds_file_path)

	def load_dds(self, file_path):
		logging.info(f"Loading DDS {file_path}")
		size_info = self.get_tex_structs()

		# load dds
		dds_file = DdsFile()
		dds_file.load(file_path)
		tex_buffers = self.header.buffer_infos.data
		if is_pc(self.ovl):
			buffer_bytes = dds_file.pack_mips_pc(tex_buffers)
		else:
			packed = dds_file.pack_mips(size_info.mip_maps)
			# slice packed bytes according to tex header buffer specifications
			buffer_bytes = [packed[b.offset: b.offset + b.size] for b in tex_buffers]
		# set data on the buffers
		for buffer_entry, b_slice in zip(self.get_sorted_streams(), buffer_bytes):
			buffer_entry.update_data(b_slice)
		# fix as we don't use the data.update_data api here
		for data_entry in self.get_sorted_datas():
			data_entry.size_1 = 0
			data_entry.size_2 = sum(buffer.size for buffer in data_entry.buffers)

	def get_sorted_datas(self):
		# lod0 | lod1 | static
		return [loader.data_entry for loader in sorted(self.streams, key=lambda f: f.file_entry.name)] + [self.data_entry, ]

	def get_sorted_streams(self):
		# PZ assigns the buffer index for the complete struct 0 | 1 | 2, 3
		# from JWE2, buffer index for streams is 0 | 0 | 0, 1
		# the last buffer is always 0 bytes
		# seen 1 buffer per stream
		# seen 2 buffer for static
		return [b for data_entry in self.get_sorted_datas() for b in data_entry.buffers]

	def get_tex_structs(self):
		if is_dla(self.ovl):
			return self.header
		if is_pc(self.ovl) or is_ztuac(self.ovl):
			# this corresponds to a stripped down size_info
			return self.header.buffer_infos.data[0]
		else:
			return self.header.size_info.data.data

	def extract(self, out_dir):
		tex_paths = super().extract(out_dir)
		tex_name = self.root_entry.name
		basename = os.path.splitext(tex_name)[0]
		dds_name = basename + ".dds"
		logging.info(f"Writing {tex_name}")

		# get joined output buffer
		buffer_data = b"".join([buffer.data for buffer in self.get_sorted_streams()])

		out_files = []
		out_files.extend(tex_paths)
		# out_files.extend(self.dump_buffers(out_dir))

		dds_file = DdsFile()
		size_info = self.get_tex_structs()

		dds_file.width = size_info.width
		dds_file.height = size_info.height
		dds_file.mipmap_count = size_info.num_mips
		if hasattr(size_info, "depth") and size_info.depth:
			dds_file.depth = size_info.depth
		if hasattr(size_info, "array_size") and size_info.array_size:
			dds_file.dx_10.array_size = size_info.array_size
		try:
			compression_name = self.header.compression_type.name
			logging.info(self.header.compression_type)
			# account for aliases
			if compression_name.endswith(("_B", "_C")):
				compression_name = compression_name[:-2]
			dds_compression_types = ((compression_name, DxgiFormat[compression_name]),)
		except KeyError:
			dds_compression_types = [(x.name, x) for x in DxgiFormat]
			logging.warning(f"Unknown compression type {self.header.compression_type}, trying all compression types")
		logging.debug(f"dds_compression_type {dds_compression_types}")

		# write out everything for each compression type
		for compression_name, compression_type in dds_compression_types:
			# header attribs
			if not is_ztuac(self.ovl):
				dds_file.width = align_to(dds_file.width, compression_name)
	
			# set compression
			dds_file.dx_10.dxgi_format = compression_type
			if not (is_ztuac(self.ovl) or is_pc(self.ovl) or is_dla(self.ovl)):
				# regenerate continous buffer data, depending on compression type
				buffer_data = dds_file.unpack_mips(size_info.mip_maps, buffer_data)
			# hack until we have proper support for array_size for texconv, after packing has been undone
			dds_file.height *= dds_file.dx_10.array_size
			dds_file.dx_10.array_size = 1
			dds_file.buffer = buffer_data
			dds_file.linear_size = len(buffer_data)
			# start out
			dds_path = out_dir(dds_name)
			if len(dds_compression_types) > 1:
				dds_path += f"_{compression_name}.dds"
	
			# write dds
			dds_file.save(dds_path)
			# print(dds_file)
			out_files.append(dds_path)
	
			# convert the dds to PNG, PNG must be visible so put it in out_dir
			png_file_path = texconv.dds_to_png(dds_path, dds_file.height)
	
			if os.path.isfile(png_file_path):
				# postprocessing of the png
				out_files.extend(imarray.wrapper(png_file_path, size_info, self.ovl))
		return out_files

	def ensure_size_match(self, png_file_path, size_info):
		"""Check that DDS files have the same basic size"""
		png_width, png_height = iio.immeta(png_file_path)["shape"]
		tex_h = size_info.height
		tex_w = size_info.width
		if hasattr(size_info, "tex_d"):
			tex_d = size_info.depth
		else:
			tex_d = 1
		tex_a = size_info.array_size
		tex_w = align_to(tex_w, self.header.compression_type.name)
		if png_width * png_height != tex_h * tex_w * tex_d * tex_a:
			raise AttributeError(
				f"Dimensions do not match for {self.file_entry.name}!\n"
				f"Dimensions: height x width x depth [array size]\n"
				f".tex file: {tex_h} x {tex_w} x {tex_d} [{tex_a}]\n"
				f".png file: {png_height} x {png_width}\n\n"
				f"Make the textures' dimensions match and try again!")
